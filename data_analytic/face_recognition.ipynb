{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfEaL1lEjrhj",
        "outputId": "81803e17-8151-4f4c-b713-89a2be150a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading AIST4010-Spring2024-A1.zip to /content\n",
            " 97% 140M/144M [00:04<00:00, 31.9MB/s]\n",
            "100% 144M/144M [00:05<00:00, 30.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"<my_username>\",\"key\":\"<my_key>\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c AIST4010-Spring2024-A1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_abcPDS1sYa7"
      },
      "outputs": [],
      "source": [
        "!unzip -q AIST4010-Spring2024-A1.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MJPw65YxjPK"
      },
      "source": [
        "Reminder: First change the runtime to a GPU runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCGKDkEayh9T"
      },
      "source": [
        "Check if the download is success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V8aJ5CdyQYx",
        "outputId": "896a5bea-b78d-4efd-d43b-b91854d3b3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__) # check torch vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fXStIjGz9xj",
        "outputId": "a3bdb189-0227-49bf-81d2-43b0f90d4928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "# determine if using gpu or cpu\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtXiKwrra87k"
      },
      "outputs": [],
      "source": [
        "path = './data/'\n",
        "\n",
        "train_annotates, val_annotates = [], []\n",
        "for dirname, _, filenames in os.walk(path + 'train'):\n",
        "    for filename in filenames:\n",
        "        train_annotates.append([os.path.join(dirname.split('/')[-1], filename), int(dirname.split('_')[-1])])\n",
        "\n",
        "for dirname, _, filenames in os.walk(path + 'val'):\n",
        "    for filename in filenames:\n",
        "        val_annotates.append([os.path.join(dirname.split('/')[-1], filename), int(dirname.split('_')[-1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv--eDkBZJnA"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.DataFrame(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKX1Pn9m7X94"
      },
      "outputs": [],
      "source": [
        "# transforms PIL to tensor\n",
        "trans = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.GaussianBlur(kernel_size=(7, 11), sigma=(0.1, 4)),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_data = CustomImageDataset(train_annotates, path + 'train', transform=trans)\n",
        "valid_data = CustomImageDataset(val_annotates, path + 'val', transform=trans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KjoPY-mbZdJ"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(valid_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUQEnQbOB0rq"
      },
      "source": [
        "# Build the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3GdmjJvB0OR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXcEh1rqGGlG"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "\n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "\n",
        "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.batch_norm3(x)\n",
        "\n",
        "        #downsample if needed\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "\n",
        "        #add identity\n",
        "        x+=identity\n",
        "        x=self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      identity = x.clone()\n",
        "\n",
        "      x = self.relu(self.batch_norm2(self.conv1(x)))\n",
        "      x = self.batch_norm2(self.conv2(x))\n",
        "\n",
        "      if self.i_downsample is not None:\n",
        "          identity = self.i_downsample(identity)\n",
        "\n",
        "      x += identity\n",
        "      x = self.relu(x)\n",
        "      return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n",
        "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
        "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
        "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        x = self.max_pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
        "            )\n",
        "\n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "\n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "def ResNet50(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arbuUY2lDUqm"
      },
      "outputs": [],
      "source": [
        "resnet50 = ResNet50(num_classes=1000, channels=1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmnXxDv1ktD3"
      },
      "source": [
        "## build the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jS7ivSqkvS-",
        "outputId": "35496508-654d-475b-9a31-5753b9099c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80, Loss: 6.3122, train_accuracy: 508/93134, 0.5454506410118753%\n",
            "Accuracy of validation images: 19/2000, 0.95%\n",
            "Epoch 2/80, Loss: 5.5856, train_accuracy: 2524/93134, 2.7100736573109714%\n",
            "Accuracy of validation images: 82/2000, 4.1%\n",
            "Epoch 3/80, Loss: 4.5627, train_accuracy: 9249/93134, 9.930852320312669%\n",
            "Accuracy of validation images: 235/2000, 11.75%\n",
            "Epoch 4/80, Loss: 4.5075, train_accuracy: 21198/93134, 22.760753323168768%\n",
            "Accuracy of validation images: 431/2000, 21.55%\n",
            "Epoch 5/80, Loss: 2.9074, train_accuracy: 31497/93134, 33.81901346447055%\n",
            "Accuracy of validation images: 567/2000, 28.35%\n",
            "Epoch 6/80, Loss: 2.9512, train_accuracy: 39906/93134, 42.847939527991926%\n",
            "Accuracy of validation images: 735/2000, 36.75%\n",
            "Epoch 7/80, Loss: 2.1031, train_accuracy: 47151/93134, 50.6270534928168%\n",
            "Accuracy of validation images: 795/2000, 39.75%\n",
            "Epoch 8/80, Loss: 1.5534, train_accuracy: 53619/93134, 57.57188567010974%\n",
            "Accuracy of validation images: 780/2000, 39.0%\n",
            "Epoch 9/80, Loss: 1.3669, train_accuracy: 58967/93134, 63.31414950501428%\n",
            "Accuracy of validation images: 912/2000, 45.6%\n",
            "Epoch 10/80, Loss: 1.1601, train_accuracy: 63338/93134, 68.00738720553181%\n",
            "Accuracy of validation images: 1028/2000, 51.4%\n",
            "Epoch 11/80, Loss: 0.5865, train_accuracy: 67548/93134, 72.52775570683102%\n",
            "Accuracy of validation images: 1044/2000, 52.2%\n",
            "Epoch 12/80, Loss: 2.0843, train_accuracy: 71414/93134, 76.6787639315395%\n",
            "Accuracy of validation images: 1112/2000, 55.6%\n",
            "Epoch 13/80, Loss: 1.0036, train_accuracy: 74611/93134, 80.11145231601778%\n",
            "Accuracy of validation images: 1114/2000, 55.7%\n",
            "Epoch 14/80, Loss: 0.7004, train_accuracy: 77477/93134, 83.18873880645091%\n",
            "Accuracy of validation images: 1150/2000, 57.5%\n",
            "Epoch 15/80, Loss: 1.0465, train_accuracy: 79891/93134, 85.7807030729916%\n",
            "Accuracy of validation images: 1130/2000, 56.5%\n",
            "Epoch 16/80, Loss: 1.2141, train_accuracy: 81937/93134, 87.97753774131895%\n",
            "Accuracy of validation images: 1201/2000, 60.05%\n",
            "Epoch 17/80, Loss: 0.1661, train_accuracy: 83789/93134, 89.9660703931969%\n",
            "Accuracy of validation images: 1155/2000, 57.75%\n",
            "Epoch 18/80, Loss: 0.4718, train_accuracy: 85362/93134, 91.65503468121202%\n",
            "Accuracy of validation images: 1180/2000, 59.0%\n",
            "Epoch 19/80, Loss: 0.1972, train_accuracy: 86601/93134, 92.98537590997917%\n",
            "Accuracy of validation images: 1183/2000, 59.15%\n",
            "Epoch 20/80, Loss: 0.2371, train_accuracy: 87310/93134, 93.7466446195804%\n",
            "Accuracy of validation images: 1144/2000, 57.2%\n",
            "Epoch 21/80, Loss: 0.6727, train_accuracy: 88272/93134, 94.77956492795327%\n",
            "Accuracy of validation images: 1203/2000, 60.15%\n",
            "Epoch 22/80, Loss: 0.6670, train_accuracy: 88866/93134, 95.41735563811282%\n",
            "Accuracy of validation images: 1208/2000, 60.4%\n",
            "Epoch 23/80, Loss: 0.0674, train_accuracy: 89260/93134, 95.84040200141732%\n",
            "Accuracy of validation images: 1201/2000, 60.05%\n",
            "Epoch 24/80, Loss: 0.2877, train_accuracy: 89764/93134, 96.38155775549208%\n",
            "Accuracy of validation images: 1212/2000, 60.6%\n",
            "Epoch 25/80, Loss: 0.4616, train_accuracy: 90013/93134, 96.64891446732665%\n",
            "Accuracy of validation images: 1165/2000, 58.25%\n",
            "Epoch 26/80, Loss: 0.0513, train_accuracy: 90428/93134, 97.09450898705092%\n",
            "Accuracy of validation images: 1249/2000, 62.45%\n",
            "Epoch 27/80, Loss: 0.1247, train_accuracy: 90742/93134, 97.43165761161337%\n",
            "Accuracy of validation images: 1178/2000, 58.9%\n",
            "Epoch 28/80, Loss: 0.4819, train_accuracy: 90819/93134, 97.51433418515258%\n",
            "Accuracy of validation images: 1205/2000, 60.25%\n",
            "Epoch 29/80, Loss: 0.0883, train_accuracy: 91106/93134, 97.8224923228896%\n",
            "Accuracy of validation images: 1255/2000, 62.75%\n",
            "Epoch 30/80, Loss: 0.1500, train_accuracy: 91179/93134, 97.9008740094917%\n",
            "Accuracy of validation images: 1274/2000, 63.7%\n",
            "Epoch 31/80, Loss: 0.0094, train_accuracy: 91411/93134, 98.14997745184358%\n",
            "Accuracy of validation images: 1249/2000, 62.45%\n",
            "Epoch 32/80, Loss: 0.2192, train_accuracy: 91649/93134, 98.40552322460111%\n",
            "Accuracy of validation images: 1272/2000, 63.6%\n",
            "Epoch 33/80, Loss: 0.3794, train_accuracy: 91703/93134, 98.46350419825198%\n",
            "Accuracy of validation images: 1250/2000, 62.5%\n",
            "Epoch 34/80, Loss: 0.2153, train_accuracy: 91865/93134, 98.63744711920458%\n",
            "Accuracy of validation images: 1227/2000, 61.35%\n",
            "Epoch 35/80, Loss: 0.0812, train_accuracy: 91941/93134, 98.71904997100951%\n",
            "Accuracy of validation images: 1255/2000, 62.75%\n",
            "Epoch 36/80, Loss: 0.0070, train_accuracy: 92064/93134, 98.85111774432538%\n",
            "Accuracy of validation images: 1277/2000, 63.85%\n",
            "Epoch 37/80, Loss: 0.0374, train_accuracy: 92154/93134, 98.94775270041016%\n",
            "Accuracy of validation images: 1261/2000, 63.05%\n",
            "Epoch 38/80, Loss: 0.2580, train_accuracy: 92311/93134, 99.11632701269139%\n",
            "Accuracy of validation images: 1259/2000, 62.95%\n",
            "Epoch 39/80, Loss: 0.0031, train_accuracy: 92285/93134, 99.08841024760024%\n",
            "Accuracy of validation images: 1318/2000, 65.9%\n",
            "Epoch 40/80, Loss: 0.0107, train_accuracy: 92426/93134, 99.23980501213306%\n",
            "Accuracy of validation images: 1290/2000, 64.5%\n",
            "Epoch 41/80, Loss: 0.0123, train_accuracy: 92461/93134, 99.27738527283269%\n",
            "Accuracy of validation images: 1286/2000, 64.3%\n",
            "Epoch 42/80, Loss: 0.0144, train_accuracy: 92516/93134, 99.33643996821783%\n",
            "Accuracy of validation images: 1294/2000, 64.7%\n",
            "Epoch 43/80, Loss: 0.0046, train_accuracy: 92691/93134, 99.52434127171603%\n",
            "Accuracy of validation images: 1308/2000, 65.4%\n",
            "Epoch 44/80, Loss: 0.0080, train_accuracy: 92548/93134, 99.37079906371464%\n",
            "Accuracy of validation images: 1308/2000, 65.4%\n",
            "Epoch 45/80, Loss: 0.0188, train_accuracy: 92651/93134, 99.48139240234501%\n",
            "Accuracy of validation images: 1313/2000, 65.65%\n",
            "Epoch 46/80, Loss: 0.0063, train_accuracy: 92761/93134, 99.5995017931153%\n",
            "Accuracy of validation images: 1295/2000, 64.75%\n",
            "Epoch 47/80, Loss: 0.0030, train_accuracy: 92740/93134, 99.57695363669552%\n",
            "Accuracy of validation images: 1300/2000, 65.0%\n",
            "Epoch 48/80, Loss: 0.1013, train_accuracy: 92795/93134, 99.63600833208066%\n",
            "Accuracy of validation images: 1290/2000, 64.5%\n",
            "Epoch 49/80, Loss: 0.0056, train_accuracy: 92822/93134, 99.6649988189061%\n",
            "Accuracy of validation images: 1329/2000, 66.45%\n",
            "Epoch 50/80, Loss: 0.0572, train_accuracy: 92903/93134, 99.75197027938239%\n",
            "Accuracy of validation images: 1316/2000, 65.8%\n",
            "Epoch 51/80, Loss: 0.0054, train_accuracy: 92911/93134, 99.7605600532566%\n",
            "Accuracy of validation images: 1310/2000, 65.5%\n",
            "Epoch 52/80, Loss: 0.1652, train_accuracy: 92925/93134, 99.77559215753645%\n",
            "Accuracy of validation images: 1340/2000, 67.0%\n",
            "Epoch 53/80, Loss: 0.0257, train_accuracy: 92949/93134, 99.80136147915906%\n",
            "Accuracy of validation images: 1346/2000, 67.3%\n",
            "Epoch 54/80, Loss: 0.0183, train_accuracy: 92974/93134, 99.82820452251595%\n",
            "Accuracy of validation images: 1345/2000, 67.25%\n",
            "Epoch 55/80, Loss: 0.0022, train_accuracy: 93027/93134, 99.88511177443254%\n",
            "Accuracy of validation images: 1330/2000, 66.5%\n",
            "Epoch 56/80, Loss: 0.0001, train_accuracy: 92993/93134, 99.84860523546718%\n",
            "Accuracy of validation images: 1342/2000, 67.1%\n",
            "Epoch 57/80, Loss: 0.0026, train_accuracy: 93034/93134, 99.89262782657246%\n",
            "Accuracy of validation images: 1350/2000, 67.5%\n",
            "Epoch 58/80, Loss: 0.0201, train_accuracy: 93028/93134, 99.88618549616682%\n",
            "Accuracy of validation images: 1338/2000, 66.9%\n",
            "Epoch 59/80, Loss: 0.0011, train_accuracy: 93052/93134, 99.91195481778942%\n",
            "Accuracy of validation images: 1353/2000, 67.65%\n",
            "Epoch 60/80, Loss: 0.0114, train_accuracy: 93058/93134, 99.91839714819507%\n",
            "Accuracy of validation images: 1361/2000, 68.05%\n",
            "Epoch 61/80, Loss: 0.0839, train_accuracy: 93052/93134, 99.91195481778942%\n",
            "Accuracy of validation images: 1362/2000, 68.1%\n",
            "Epoch 62/80, Loss: 0.0002, train_accuracy: 93074/93134, 99.93557669594348%\n",
            "Accuracy of validation images: 1381/2000, 69.05%\n",
            "Epoch 63/80, Loss: 0.0064, train_accuracy: 93098/93134, 99.96134601756609%\n",
            "Accuracy of validation images: 1376/2000, 68.8%\n",
            "Epoch 64/80, Loss: 0.0072, train_accuracy: 93084/93134, 99.94631391328623%\n",
            "Accuracy of validation images: 1365/2000, 68.25%\n",
            "Epoch 65/80, Loss: 0.0059, train_accuracy: 93118/93134, 99.98282045225159%\n",
            "Accuracy of validation images: 1373/2000, 68.65%\n",
            "Epoch 66/80, Loss: 0.0004, train_accuracy: 93101/93134, 99.96456718276892%\n",
            "Accuracy of validation images: 1348/2000, 67.4%\n",
            "Epoch 67/80, Loss: 0.0003, train_accuracy: 93108/93134, 99.97208323490884%\n",
            "Accuracy of validation images: 1381/2000, 69.05%\n",
            "Epoch 68/80, Loss: 0.0057, train_accuracy: 93124/93134, 99.98926278265725%\n",
            "Accuracy of validation images: 1373/2000, 68.65%\n",
            "Epoch 69/80, Loss: 0.0013, train_accuracy: 93111/93134, 99.97530440011167%\n",
            "Accuracy of validation images: 1383/2000, 69.15%\n",
            "Epoch 70/80, Loss: 0.0001, train_accuracy: 93120/93134, 99.98496789572015%\n",
            "Accuracy of validation images: 1371/2000, 68.55%\n",
            "Epoch 71/80, Loss: 0.0016, train_accuracy: 93124/93134, 99.98926278265725%\n",
            "Accuracy of validation images: 1357/2000, 67.85%\n",
            "Epoch 72/80, Loss: 0.0002, train_accuracy: 93121/93134, 99.98604161745442%\n",
            "Accuracy of validation images: 1384/2000, 69.2%\n",
            "Epoch 73/80, Loss: 0.0010, train_accuracy: 93124/93134, 99.98926278265725%\n",
            "Accuracy of validation images: 1359/2000, 67.95%\n",
            "Epoch 74/80, Loss: 0.0051, train_accuracy: 93123/93134, 99.98818906092298%\n",
            "Accuracy of validation images: 1369/2000, 68.45%\n",
            "Epoch 75/80, Loss: 0.0232, train_accuracy: 93125/93134, 99.99033650439152%\n",
            "Accuracy of validation images: 1372/2000, 68.6%\n",
            "Epoch 76/80, Loss: 0.0035, train_accuracy: 93127/93134, 99.99248394786007%\n",
            "Accuracy of validation images: 1382/2000, 69.1%\n",
            "Epoch 77/80, Loss: 0.0006, train_accuracy: 93127/93134, 99.99248394786007%\n",
            "Accuracy of validation images: 1392/2000, 69.6%\n",
            "Epoch 78/80, Loss: 0.1156, train_accuracy: 93130/93134, 99.9957051130629%\n",
            "Accuracy of validation images: 1392/2000, 69.6%\n",
            "Epoch 79/80, Loss: 0.0001, train_accuracy: 93127/93134, 99.99248394786007%\n",
            "Accuracy of validation images: 1382/2000, 69.1%\n",
            "Epoch 80/80, Loss: 0.0023, train_accuracy: 93132/93134, 99.99785255653146%\n",
            "Accuracy of validation images: 1391/2000, 69.55%\n"
          ]
        }
      ],
      "source": [
        "EPOCH = 80\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(resnet50.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCH, eta_min=0.0001)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "\n",
        "    train_total = 0\n",
        "    train_correct = 0\n",
        "\n",
        "    resnet50.train()\n",
        "    for i, (image, label) in enumerate(train_dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # retrieve image and label\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # forward step\n",
        "        output = resnet50(image)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        # optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "        train_total += label.size(0)\n",
        "        train_correct += (predicted == label).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{EPOCH}, Loss: {loss.item():.4f}, train_accuracy: {train_correct}/{train_total}, {100 * train_correct/train_total}%')\n",
        "\n",
        "    # validate\n",
        "    resnet50.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for image_val, label_val in val_dataloader:\n",
        "            image_val = image_val.to(device)\n",
        "            label_val = label_val.to(device)\n",
        "\n",
        "            output_val = resnet50(image_val)\n",
        "\n",
        "            _, predicted_val = torch.max(output_val.data, 1)\n",
        "\n",
        "            total += label_val.size(0)\n",
        "            correct += (predicted_val == label_val).sum().item()\n",
        "\n",
        "\n",
        "        print(f'Accuracy of validation images: {correct}/{total}, {100 * correct/total}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P315KR06bqtq"
      },
      "outputs": [],
      "source": [
        "submission = []\n",
        "\n",
        "resnet50.eval()\n",
        "with torch.no_grad():\n",
        "    for filename in np.loadtxt('./data/test_list.txt', dtype='str'):\n",
        "        dirname = './data/test'\n",
        "        test_img = read_image(os.path.join(dirname, filename))\n",
        "\n",
        "        trans2 = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "        test_img = trans2(test_img).unsqueeze(0).to(device)\n",
        "        test_output = resnet50(test_img)\n",
        "\n",
        "        _, predicted = torch.max(test_output.data, 1)\n",
        "\n",
        "        submission.append([filename, 'a1_'+str(predicted.item())])\n",
        "\n",
        "submission = pd.DataFrame(submission, columns=['id', 'label'])\n",
        "submission.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFLnUtboiHDB",
        "outputId": "cfb96179-7428-4149-d32c-84094aff27f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.7 / client 1.6.6)\n",
            "100% 47.8k/47.8k [00:00<00:00, 74.9kB/s]\n",
            "Successfully submitted to AIST4010-Spring2024-A1"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c <competition> -f submission.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPDjuHbR7U-r"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
